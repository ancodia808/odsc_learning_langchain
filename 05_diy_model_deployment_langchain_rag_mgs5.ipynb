{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269a9b6b-e8b6-49da-aeda-b2b3e38bbf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original documents: 1\n",
      "Number of split chunks: 4920\n",
      "\n",
      "split_docs[0].page_content: In Metal Gear Solid V: The Phantom Pain, the player can access cassette tapes, either at hand or by\n",
      "\n",
      "split_docs[0].metadata: {'source': 'txt/mgs5_cassette_tapes.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load Documents\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "## Load Text\n",
    "loader = TextLoader(\"txt/mgs5_cassette_tapes.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "## Split Text\n",
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "# chunk_size: The maximum size of each chunk (in characters by default).\n",
    "# chunk_overlap: The number of characters to overlap between consecutive chunks,\n",
    "#                helping to maintain context.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,  # Use character length for chunk size\n",
    "    is_separator_regex=False, # Treat separators literally\n",
    ")\n",
    "\n",
    "# Split the loaded documents\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print the resulting chunks\n",
    "print(f\"Number of original documents: {len(documents)}\")\n",
    "print(f\"Number of split chunks: {len(split_docs)}\\n\")\n",
    "\n",
    "#for i, chunk in enumerate(split_docs):\n",
    "#    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n---\")\n",
    "print(f\"split_docs[0].page_content: {split_docs[0].page_content}\\n\")\n",
    "print(f\"split_docs[0].metadata: {split_docs[0].metadata}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8f3a92-18ed-49c8-b937-483cbc5aa7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to OCI embeddings and generative AI.\n"
     ]
    }
   ],
   "source": [
    "### Connect to OCI embeddings and generative AI\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "from langchain_community.chat_models import ChatOCIGenAI\n",
    "\n",
    "oci_embeddings = OCIGenAIEmbeddings(\n",
    "    model_id=\"cohere.embed-english-light-v3.0\",\n",
    "    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n",
    "    compartment_id=\"ocid1.compartment.oc1..aaaaaaaa52sp42nqmtwwzzvmp5mmldri26razhrbyw7cvixmims7p5crsg7a\",\n",
    ")\n",
    "\n",
    "oci_chat = ChatOCIGenAI(\n",
    "    model_id=\"cohere.command-a-03-2025\",\n",
    "    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n",
    "    compartment_id=\"ocid1.compartment.oc1..aaaaaaaa52sp42nqmtwwzzvmp5mmldri26razhrbyw7cvixmims7p5crsg7a\",\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_tokens\": 500},\n",
    ")\n",
    "\n",
    "print(\"Connected to OCI embeddings and generative AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfdb51-7913-40e6-9f86-e595acf00a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embed Documents\n",
    "#l = len(split_docs)\n",
    "#embeddings = []\n",
    "#for i in range(l // 16 + 1):\n",
    "#    #print(f\"Embedding from index {i*16} to {(i + 1) * 16}...\")\n",
    "#    subdocs = [item.page_content for item in split_docs[i * 16: (i + 1) * 16]]\n",
    "#    embeddings.extend(oci_embeddings.embed_documents(subdocs))\n",
    "#\n",
    "#print(\"Documents embedded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84050aa-5d67-449d-98d7-1ce0b3e0efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build Vector Store (FAISS)\n",
    "#from langchain_community.vectorstores import FAISS\n",
    "#\n",
    "#texts = [item.page_content for item in split_docs]\n",
    "#text_embedding_pairs = [(text, embed) for text, embed in zip(texts, embeddings)]\n",
    "#vectorstore = FAISS.from_embeddings(text_embedding_pairs, oci_embeddings)\n",
    "#\n",
    "#print(\"FAISS vector store built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0671a-eda9-4b1a-ab6c-8ff7c6cbc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.vectorstores import Chroma\n",
    "#from langchain_core.documents import Document\n",
    "#\n",
    "## 2. Define some documents to add to the vector store\n",
    "#documents = [\n",
    "#    Document(page_content=\"The quick brown fox jumps over the lazy dog.\", metadata={\"source\": \"fable\"}),\n",
    "#    Document(page_content=\"Artificial intelligence is transforming industries worldwide.\", metadata={\"source\": \"technology\"}),\n",
    "#    Document(page_content=\"The capital of France is Paris.\", metadata={\"source\": \"geography\"}),\n",
    "#]\n",
    "#\n",
    "## 3. Create a Chroma vector store instance\n",
    "## You can specify a persist_directory to save the data to disk\n",
    "#persist_directory = \"./chroma_db\"\n",
    "#vectorstore = Chroma.from_documents(\n",
    "#    documents=documents,\n",
    "#    embedding=embeddings,\n",
    "#    persist_directory=persist_directory,\n",
    "#    collection_name=\"my_documents\"\n",
    "#)\n",
    "#print(\"Chroma vector store built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80071137-6575-4d4d-8818-a7a8b7110f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Build Vector Store (In-Memory)\n",
    "#from langchain_core.vectorstores import InMemoryVectorStore\n",
    "#\n",
    "## Create an in-memory vector store\n",
    "#vectorstore = InMemoryVectorStore(oci_embeddings)\n",
    "#vectorstore.add_documents(documents=split_docs)\n",
    "#\n",
    "#print(\"In-memory vector store built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0928b9-3f8a-450f-ba37-66dad3d3c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store built.\n"
     ]
    }
   ],
   "source": [
    "### Build Vector Store (FAISS)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(split_docs, oci_embeddings)\n",
    "\n",
    "print(\"FAISS vector store built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a178f33e-b6dd-43b6-ada9-0e9865319f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain built.\n"
     ]
    }
   ],
   "source": [
    "### Build Chain (OCI chat-based Retrieval QA)\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_classic.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectorstore.as_retriever(retriever_kwargs={\"similarity_top_k\": 3})\n",
    "\n",
    "#rag_prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "#{context}\n",
    "#Question: {question}\n",
    "#\"\"\"\n",
    "\n",
    "#rag_prompt_template = \"\"\"Try to the question using only the following context, but, if that fails, use your general knowledge.  You don't have to mention whether you did or did not use the context.\n",
    "#{context}\n",
    "#Question: {question}\n",
    "#\"\"\"\n",
    "\n",
    "rag_prompt_template = \"\"\"Try to the question using only the following context, but, if that fails, use your general knowledge.\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=oci_chat,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt,},\n",
    ")\n",
    "\n",
    "#rag = RetrievalQA.from_chain_type(\n",
    "#    llm=oci_chat,\n",
    "#    chain_type=\"stuff\",\n",
    "#    retriever=retriever,\n",
    "#    return_source_documents=True,\n",
    "#    verbose=False,\n",
    "#)\n",
    "\n",
    "print(\"RAG chain built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80248d8e-4949-4be4-8490-a9f5d1ef30b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Who is Kazuhira?', 'result': 'Based on the provided context, Kazuhira is Benedict Miller, referred to as Benedict \"Kazuhira\" Miller. The context mentions his research and epiphany related to \"The Hamburgers of Kazuhira Miller,\" but it does not provide further details about his identity beyond his name. \\n\\nSince the context does not offer additional information, I cannot provide more details using only the given text. However, using general knowledge (as the context fails to provide a complete answer), Kazuhira Miller is a character from the *Metal Gear* video game series, specifically from *Metal Gear Solid V: The Phantom Pain*. He is a mentor figure to the protagonist, Venom Snake, and plays a significant role in the game\\'s story.'}\n",
      "{'query': 'Are Venom Snake and Revolver Ocelot friends?', 'result': \"Based on the provided context, it is not explicitly stated whether Venom Snake and Revolver Ocelot are friends. The dialogue suggests a level of familiarity and a working relationship, but it does not confirm a friendship. They seem to be discussing a plan or strategy, and Ocelot mentions his preference to work alone, which could imply a professional rather than a personal relationship.\\n\\nHowever, using general knowledge, in the *Metal Gear* series, Venom Snake and Revolver Ocelot have a complex and evolving relationship. While they are allies and work together at times, their relationship is often marked by manipulation, deception, and shifting loyalties, especially given Ocelot's role as a double (or even triple) agent. Their bond is more of a tactical alliance than a straightforward friendship.\"}\n",
      "{'query': \"What is the name of Anderson's AI?\", 'result': 'Based on the provided context, the name of Anderson\\'s AI is **\"The Patriots.\"**'}\n",
      "{'query': \"What is Zero's fear once the Cold War is over?\", 'result': \"Based on the provided context, Zero's fear once the Cold War is over is that their enemies won't be so clearly defined.\"}\n",
      "{'query': 'How many cassette tapes are there in Metal Gear Solid 5?', 'result': 'Based on the provided context, there are **151 cassette tapes** in Metal Gear Solid V.'}\n"
     ]
    }
   ],
   "source": [
    "### Invoke the chain (unit test)\n",
    "print(rag.invoke(\"Who is Kazuhira?\"))\n",
    "print(rag.invoke(\"Are Venom Snake and Revolver Ocelot friends?\"))\n",
    "print(rag.invoke(\"What is the name of Anderson's AI?\"))\n",
    "print(rag.invoke(\"What is Zero's fear once the Cold War is over?\"))\n",
    "print(rag.invoke(\"How many cassette tapes are there in Metal Gear Solid 5?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cab1a17-b164-4bb3-b166-0d207af9a8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Actions Needed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Details</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>initiate</th>\n",
       "      <th>Done</th>\n",
       "      <th>Initiated the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">prepare()</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">Available</th>\n",
       "      <th>Generated runtime.yaml</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Generated score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serialized model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Populated metadata(Custom, Taxonomy and Provenance)</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verify()</th>\n",
       "      <th>Not Available</th>\n",
       "      <th>Local tested .predict from score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">save()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Not Available</th>\n",
       "      <th>Conducted Introspect Test</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uploaded artifact to model catalog</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deploy()</th>\n",
       "      <th>UNKNOWN</th>\n",
       "      <th>Deployed the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict()</th>\n",
       "      <th>Not Available</th>\n",
       "      <th>Called deployment predict endpoint</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            Actions Needed\n",
       "Step      Status        Details                                                           \n",
       "initiate  Done          Initiated the model                                               \n",
       "prepare() Available     Generated runtime.yaml                                            \n",
       "                        Generated score.py                                                \n",
       "                        Serialized model                                                  \n",
       "                        Populated metadata(Custom, Taxonomy and Provenance)               \n",
       "verify()  Not Available Local tested .predict from score.py                               \n",
       "save()    Not Available Conducted Introspect Test                                         \n",
       "                        Uploaded artifact to model catalog                                \n",
       "deploy()  UNKNOWN       Deployed the model                                                \n",
       "predict() Not Available Called deployment predict endpoint                                "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Use ADS to deploy the unit-tested chain\n",
    "## Create the ADS deployment object\n",
    "import tempfile\n",
    "from ads.llm.deploy import ChainDeployment\n",
    "\n",
    "artifact_dir = tempfile.mkdtemp()\n",
    "\n",
    "ads_deployment = ChainDeployment(\n",
    "    chain=rag,\n",
    "    artifact_dir=artifact_dir,\n",
    "    force_overwrite=True\n",
    ")\n",
    "\n",
    "ads_deployment.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce986b5a-f2ce-4e00-9100-78e4f52fd232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/automlx251_p311_cpu_x86_64_v2/lib/python3.11/site-packages/ads/model/runtime/env_info.py:92: UserWarning: slug will be deprecated. Provide conda pack path instead.s]\n",
      "  warnings.warn(\"slug will be deprecated. Provide conda pack path instead.\")\n",
      "\n",
      "ERROR:root:Error occurred in attempt to extract the list of the service conda environments from the object storage for bucket 'service-conda-packs' and namespace 'id19sfcrra6z'. Please make sure that you've provided correct bucket and namespace.\n",
      "INFO:ADS:To auto-extract taxonomy metadata the model must be provided. Supported models: keras, lightgbm, pytorch, sklearn, tensorflow, pyspark, and xgboost.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Actions Needed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Details</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>initiate</th>\n",
       "      <th>Done</th>\n",
       "      <th>Initiated the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">prepare()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Done</th>\n",
       "      <th>Generated runtime.yaml</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Generated score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Needs Action</th>\n",
       "      <th>Serialized model</th>\n",
       "      <td>Model is not automatically serialized. Serialize the model as `model.pkl` and save to the /tmp/tmp3aojz77n.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Done</th>\n",
       "      <th>Populated metadata(Custom, Taxonomy and Provenance)</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verify()</th>\n",
       "      <th>Available</th>\n",
       "      <th>Local tested .predict from score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">save()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Available</th>\n",
       "      <th>Conducted Introspect Test</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uploaded artifact to model catalog</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deploy()</th>\n",
       "      <th>UNKNOWN</th>\n",
       "      <th>Deployed the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict()</th>\n",
       "      <th>Not Available</th>\n",
       "      <th>Called deployment predict endpoint</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                          Actions Needed\n",
       "Step      Status        Details                                                                                                                                                         \n",
       "initiate  Done          Initiated the model                                                                                                                                             \n",
       "prepare() Done          Generated runtime.yaml                                                                                                                                          \n",
       "                        Generated score.py                                                                                                                                              \n",
       "          Needs Action  Serialized model                                     Model is not automatically serialized. Serialize the model as `model.pkl` and save to the /tmp/tmp3aojz77n.\n",
       "          Done          Populated metadata(Custom, Taxonomy and Provenance)                                                                                                             \n",
       "verify()  Available     Local tested .predict from score.py                                                                                                                             \n",
       "save()    Available     Conducted Introspect Test                                                                                                                                       \n",
       "                        Uploaded artifact to model catalog                                                                                                                              \n",
       "deploy()  UNKNOWN       Deployed the model                                                                                                                                              \n",
       "predict() Not Available Called deployment predict endpoint                                                                                                                              "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the ADS deployment\n",
    "ads_deployment.prepare(\n",
    "    inference_conda_env=\"automlx251_p311_cpu_x86_64_v2\",\n",
    "    inference_python_version=\"3.11\",\n",
    "    force_overwrite=True,\n",
    ")\n",
    "\n",
    "# Summarize the checkpoitn ADS workflow status\n",
    "ads_deployment.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e1e3108-8e2f-4367-8e17-37585bc1955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/automlx251_p311_cpu_x86_64_v2/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_16147/267121191.py\", line 2, in <module>\n",
      "    ads_deployment.verify(\"Who wants to cook hamburgers once the conflict is over?\")\n",
      "  File \"/home/datascience/conda/automlx251_p311_cpu_x86_64_v2/lib/python3.11/site-packages/ads/model/generic_model.py\", line 1277, in verify\n",
      "    raise ArtifactsNotAvailableError\n",
      "ads.model.generic_model.ArtifactsNotAvailableError: Model artifacts are either not generated or not available locally.\n",
      "ArtifactsNotAvailableError: Model artifacts are either not generated or not available locally."
     ]
    }
   ],
   "source": [
    "# Save the ADS model\n",
    "ads_deployment.verify(\"Who wants to cook hamburgers once the conflict is over?\")\n",
    "\n",
    "# Summarize the checkpoitn ADS workflow status\n",
    "ads_deployment.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ae250db-ccca-4999-b1ea-3cf3271be72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/automlx251_p311_cpu_x86_64_v2/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_16147/853832723.py\", line 2, in <module>\n",
      "    model_id = ads_deployment.save()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/datascience/conda/automlx251_p311_cpu_x86_64_v2/lib/python3.11/site-packages/ads/model/generic_model.py\", line 2134, in save\n",
      "    raise ArtifactsNotAvailableError\n",
      "ads.model.generic_model.ArtifactsNotAvailableError: Model artifacts are either not generated or not available locally.\n",
      "ArtifactsNotAvailableError: Model artifacts are either not generated or not available locally."
     ]
    }
   ],
   "source": [
    "# Save the ADS model\n",
    "model_id = ads_deployment.save()\n",
    "\n",
    "# Summarize the checkpoitn ADS workflow status\n",
    "ads_deployment.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051f689-68c2-4e8a-b481-d3ccae737f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:automlx251_p311_cpu_x86_64_v2]",
   "language": "python",
   "name": "conda-env-automlx251_p311_cpu_x86_64_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
