{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269a9b6b-e8b6-49da-aeda-b2b3e38bbf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original documents: 3\n",
      "Number of split chunks: 588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ads\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from ads.llm.deploy import ChainDeployment\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.chat_models import ChatOCIGenAI\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "ads.set_auth(auth=\"resource_principal\")\n",
    "\n",
    "### Load Documents\n",
    "file_extension = \".pptx\"\n",
    "directory_path = \"/home/datascience/demo/pptx\"\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(file_extension) and os.path.isfile(file_path):\n",
    "        loader = UnstructuredPowerPointLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "# The 'data' variable now holds a list of LangChain Document objects\n",
    "# You can inspect the content and metadata of the documents:\n",
    "#print(documents[0].page_content)\n",
    "#print(documents[0].metadata)\n",
    "\n",
    "## Split Text\n",
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "# chunk_size: The maximum size of each chunk (in characters by default).\n",
    "# chunk_overlap: The number of characters to overlap between consecutive chunks,\n",
    "#                helping to maintain context.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,  # Use character length for chunk size\n",
    "    is_separator_regex=False, # Treat separators literally\n",
    ")\n",
    "\n",
    "# Split the loaded documents\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print the resulting chunks\n",
    "print(f\"Number of original documents: {len(documents)}\")\n",
    "print(f\"Number of split chunks: {len(split_docs)}\\n\")\n",
    "\n",
    "#for i, chunk in enumerate(split_docs):\n",
    "#    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8f3a92-18ed-49c8-b937-483cbc5aa7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to OCI embeddings and generative AI.\n"
     ]
    }
   ],
   "source": [
    "### Connect to OCI embeddings and generative AI\n",
    "\n",
    "oci_embeddings = OCIGenAIEmbeddings(\n",
    "    model_id=\"cohere.embed-english-light-v3.0\",\n",
    "    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n",
    "    compartment_id=\"ocid1.compartment.oc1..aaaaaaaa52sp42nqmtwwzzvmp5mmldri26razhrbyw7cvixmims7p5crsg7a\",\n",
    ")\n",
    "\n",
    "oci_chat = ChatOCIGenAI(\n",
    "    model_id=\"cohere.command-a-03-2025\",\n",
    "    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n",
    "    compartment_id=\"ocid1.compartment.oc1..aaaaaaaa52sp42nqmtwwzzvmp5mmldri26razhrbyw7cvixmims7p5crsg7a\",\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_tokens\": 500},\n",
    ")\n",
    "\n",
    "print(\"Connected to OCI embeddings and generative AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84050aa-5d67-449d-98d7-1ce0b3e0efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "FAISS vector store built.\n"
     ]
    }
   ],
   "source": [
    "### Build Vector Store (FAISS)\n",
    "vectorstore = FAISS.from_documents(split_docs, oci_embeddings)\n",
    "\n",
    "print(\"FAISS vector store built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a178f33e-b6dd-43b6-ada9-0e9865319f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain built.\n"
     ]
    }
   ],
   "source": [
    "### Build Chain (OCI chat-based Retrieval QA)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#rag_prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "#{context}\n",
    "#Question: {question}\n",
    "#\"\"\"\n",
    "\n",
    "#rag_prompt_template = \"\"\"Try to the question using only the following context, but, if that fails, use your general knowledge.  You don't have to mention whether you did or did not use the context.\n",
    "#{context}\n",
    "#Question: {question}\n",
    "#\"\"\"\n",
    "\n",
    "rag_prompt_template = \"\"\"Try to the question using only the following context, but, if that fails, use your general knowledge.\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate(template=rag_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "#rag = RetrievalQA.from_chain_type(\n",
    "#    llm=oci_chat,\n",
    "#    retriever=retriever,\n",
    "#    chain_type_kwargs={\"prompt\": rag_prompt,},\n",
    "#)\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=oci_chat,\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "print(\"RAG chain built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13be5fa0-0d0d-4e5a-b9ff-fe12c34bfc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the 4 pillars of performance?', 'result': 'The 4 pillars of performance, as outlined in the provided context, are:\\n\\n1. **Growth**  \\n2. **Productivity**  \\n3. **Health**  \\n4. **Innovation**  \\n\\nThese pillars form the basis of the \"Evolving 4-Pillar Approach\" for evaluating or driving performance.'}\n",
      "{'query': 'Why is Productivity important?', 'result': \"Productivity is important because it measures an individual's contribution to revenue. This metric helps assess how effectively an employee is generating value for the organization, which is crucial for understanding performance and making informed decisions about resource allocation and growth strategies.\"}\n",
      "{'query': 'Can I ignore any of the 4 pillars of performance?', 'result': 'Based on the provided context, it seems that the 4 pillars of performance (Growth, Productivity, Health, and Innovation) are presented as an \"Evolving 4-Pillar Approach,\" which implies that they are interconnected and work together to drive overall performance.\\n\\nThere is no explicit information suggesting that any of the pillars can be ignored. In fact, the repetition of the 4 pillars in the context emphasizes their importance and suggests that they are all crucial components of the performance criteria.\\n\\nTherefore, it would not be advisable to ignore any of the 4 pillars, as doing so could potentially compromise the overall performance and undermine the effectiveness of the approach. Each pillar likely plays a unique and essential role in achieving success, and neglecting any one of them could have negative consequences.\\n\\nIf you have a specific reason for considering ignoring one of the pillars, it may be helpful to re-evaluate your goals and priorities in light of the 4-pillar approach, rather than disregarding a pillar altogether.'}\n",
      "{'query': 'Which customers has Yudhvir worked with?', 'result': 'The provided context does not specify which customers Yudhvir has worked with. It only mentions that Yudhvir is the OIC (likely Oracle Integration Cloud) lead for AI/ML and Cloud-Native technologies and that he completes tasks to high customer satisfaction. Without additional information, I cannot provide details about specific customers.'}\n"
     ]
    }
   ],
   "source": [
    "### Invoke the chain (unit test)\n",
    "print(rag.invoke(\"What are the 4 pillars of performance?\"))\n",
    "print(rag.invoke(\"Why is Productivity important?\"))\n",
    "print(rag.invoke(\"Can I ignore any of the 4 pillars of performance?\"))\n",
    "print(rag.invoke(\"Which customers has Yudhvir worked with?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0adc8c1e-a5df-4d07-b6a2-f8cdf3dd42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Use ADS to deploy the unit-tested chain\n",
    "### Create the ADS deployment object\n",
    "#artifact_dir = tempfile.mkdtemp()\n",
    "#\n",
    "#ads_deployment = ChainDeployment(\n",
    "#    chain=rag,\n",
    "#    artifact_dir=artifact_dir,\n",
    "#    force_overwrite=True\n",
    "#)\n",
    "#\n",
    "#ads_deployment.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b2d73e-63bf-42d1-bc0b-22ce9e3acba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the ADS deployment\n",
    "#ads_deployment.prepare(\n",
    "#    inference_conda_env=\"python_p312_any_x86_64_v1\",\n",
    "#    inference_python_version=\"3.12\",\n",
    "#)\n",
    "#\n",
    "## Summarize the checkpoitn ADS workflow status\n",
    "#ads_deployment.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3ee04-cb74-4540-8276-8f260e9b1c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:automlx251_p311_cpu_x86_64_v2]",
   "language": "python",
   "name": "conda-env-automlx251_p311_cpu_x86_64_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
